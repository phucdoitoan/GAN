# architecture reference: 
# https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN
# https://github.com/lyeoni/pytorch-mnist-GAN

# implementing Vanilla GAN on MNIST

# 1. libraries loading
# 2. data prepare + batch_size + num of iteration + epochs
# 3. defining Generator and Discriminator models 
# 4. instantiate Generator and Discriminator + loss function 
#    + learning_rate + optimizers
# 5. training D and G
# 6. save loss, D G parameters to csv files
# 7. visualize samples generated by G

# 1. libraries loading

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.utils.data
from torch.autograd import Variable
from sklearn.model_selection import train_test_split
from torchvision.utils import save_image

# 2. data loading + batch_size + epochs + batch_num

# loading data + normalize pixels' values between -1 and +1
data = pd.read_csv('input_pytorch/mnist-all-70k.csv')
#labels = data.label.to_numpy()
img_digits = (data.loc[:, data.columns != 'label'].to_numpy()/255 - 0.5)*2

# batch_size + iteration num
batch_size = 128
epochs_num = 200

# change DataFrame to numpy
digits_Tensor = torch.Tensor(img_digits)

# build Dataset
digits_DataSet = torch.utils.data.TensorDataset(digits_Tensor)

# build DataLoader
digits_DataLoader = torch.utils.data.DataLoader(digits_DataSet, batch_size = batch_size)

batch_num = len(digits_DataLoader) # numbers of mini batches

# define functions to return mini batch of real data, noise
# sequentially return mini batch of real data from DataLoader after each call
def sample_real():
    while True:
        for i, [batch] in enumerate(digits_DataLoader):
            yield batch
sample_real = sample_real() # sample_real now is a generator

# randomly return a batch of noise
def sample_noise(size=batch_size):
    batch = torch.from_numpy(np.random.randn(size,100)).type(torch.FloatTensor)
    return batch

# 3. defining Generator and Discriminator

# generator: 100 -> 256 -> 512 -> 1024 -> 784
class Generator(nn.Module):
    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):
        super(Generator, self).__init__()
        
        # 1st layer
        self.fc1 = nn.Linear(input_dim, hidden_dim1)
        self.ac1 = nn.LeakyReLU(negative_slope=0.2)
        # 2nd layer
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.ac2 = nn.LeakyReLU(negative_slope=0.2)
        # 3rd layer
        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)
        self.ac3 = nn.LeakyReLU(negative_slope=0.2)
        # 4th layer (readout)
        self.fc4 = nn.Linear(hidden_dim3, output_dim)
        self.ac4 = nn.Tanh()
        
    def forward(self, x):
        out1 = self.ac1(self.fc1(x))
        out2 = self.ac2(self.fc2(out1))
        out3 = self.ac3(self.fc3(out2))
        out = self.ac4(self.fc4(out3))
        
        return out
    
# discriminator: 784 -> 1024 -> 512 -> 256 -> 1
class Discriminator(nn.Module):
    def __init__(self,input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):
        super(Discriminator, self).__init__()
        
        # 1st layer
        self.fc1 = nn.Linear(input_dim, hidden_dim1)
        self.ac1 = nn.LeakyReLU(negative_slope=0.2)
        self.dropout1 = nn.Dropout(p=0.3)
        # 2nd layer
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.ac2 = nn.LeakyReLU(negative_slope=0.2)
        self.dropout2 = nn.Dropout(p=0.3)
        # 3rd layer
        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)
        self.ac3 = nn.LeakyReLU(negative_slope=0.2)
        self.dropout3 = nn.Dropout(p=0.3)
        # 4th layer (readout)
        self.fc4 = nn.Linear(hidden_dim3, output_dim)
        self.ac4 = nn.Sigmoid()
        
    def forward(self, x):
        out1 = self.dropout1(self.ac1(self.fc1(x)))
        out2 = self.dropout2(self.ac2(self.fc2(out1)))
        out3 = self.dropout3(self.ac3(self.fc3(out2)))
        out = self.ac4(self.fc4(out3))
        
        return out

# 4. instantiate G and D; loss function; learning_rate; optimizers

# generator + discriminator
G = Generator(100, 256, 512, 1024, 784)
D = Discriminator(784, 1024, 512, 256, 1)

# loss function: Binary Cross Entropy Loss
error = nn.BCELoss()

# learning rate
learning_rate = 0.0002

# optimizers: 
G_optimizer = torch.optim.Adam(G.parameters(), lr = learning_rate)
D_optimizer = torch.optim.Adam(D.parameters(), lr = learning_rate)

# 5. training D and G

from timeit import default_timer as timer
import time

# list to store loss, iter
G_loss_list = []
D_real_loss_list = []
D_fake_loss_list = []
iter_list = []

# function to save digit images generated by G after each epoch
def G_generate(epoch, size=batch_size):
    generated = G(sample_noise(size))
    save_image(generated.view(generated.size(0), 1, 28, 28), 'results/epoch' + str(epoch) + '.png')

all_start = timer()    
for epoch in range(epochs_num):
    epoch_start = timer()
    for it, [real_batch] in enumerate(digits_DataLoader):
        
        ##### TRAIN D  #####
        # on real batch
        # clear gradients
        D_optimizer.zero_grad()
        # forward propagation
        real_out = D(real_batch)
        # soft label for real batch: 1
        real_label = torch.ones(real_out.shape[0], 1).type(torch.FloatTensor)
        # loss
        D_real_loss = error(real_out, real_label)
        # back propagation
        D_real_loss.backward()
        # update parameters
        D_optimizer.step()
    
        # on fake batch
        # sample noise + generate fake batch
        noise_batch = sample_noise()
        fake_batch = G(noise_batch)
        # clear gradients
        D_optimizer.zero_grad()
        # forward propagation
        fake_out = D(fake_batch)
        # soft label for fake batch: 0
        fake_label = torch.zeros(fake_out.shape[0], 1).type(torch.FloatTensor)
        # loss
        D_fake_loss = error(fake_out, fake_label)
        # back propagation
        D_fake_loss.backward()
        # update parameters
        D_optimizer.step()        
        
        ##### TRAIN G #####
        # sample noise + generate fake batch
        noise_batch = sample_noise()
        fake_batch = G(noise_batch)
        # clear gradients
        G_optimizer.zero_grad()
        # forward propagation
        fake_out = D(fake_batch)
        # soft label for fake batch: 1
        fake_label = torch.ones(fake_out.shape[0], 1).type(torch.FloatTensor)
        # loss 
        G_loss = error(fake_out, fake_label)
        # back propagation
        G_loss.backward()
        # update parameters
        G_optimizer.step()
        
        ##### store loss, it #####
        if it % 200 == 0:
            iter_list.append(it+batch_num*epoch)
            G_loss_list.append(G_loss.item())
            D_fake_loss_list.append(D_fake_loss.item())
            D_real_loss_list.append(D_real_loss.item())
        
    print('Epoch {} :'.format(epoch))
    print('    G_loss: {}'.format(G_loss.item()))
    print('    D_fake_loss: {}'.format(D_fake_loss.item())
    print('    D_real_loss: {}'.format(D_real_loss.item()))
            
    epoch_duration = timer() - epoch_start
    G_generate(epoch)
    print('##### FINISH EPOCH {} IN {} s #####'.format(epoch, epoch_duration))

all_duration = timer() - all_start
print('ALL TRAINING TIME: {}'.format(all_duration))

# 6. save loss, D G parameters to csv files

# acidentally save loss as tensor(value) => convert back to data
def tensor2number(l):
    for i in range(len(l)):
        l[i] = l[i].item()

#tensor2number(D_fake_loss_list)
#tensor2number(D_real_loss_list)
#tensor2number(G_loss_list)

# save loss
def save_loss(iter_list, G_loss_list, D_fake_loss_list, D_real_loss_list, path):
    pd.DataFrame({'iteration': iter_list, 'G_loss': G_loss_list, 'D_fake_loss': D_fake_loss_list, 'D_real_loss': D_real_loss_list}).to_csv(path, index = False)
    
save_loss(iter_list, G_loss_list, D_fake_loss_list, D_real_loss_list, 'results/loss.csv')

# save parameters
def save_parameters(paras, path):    
    for i, para in enumerate(paras):
        pd.DataFrame(para.detach().numpy()).to_csv(path + 'para_' + str(i) + '.csv', index = False)
        
# save paras of trained G
save_parameters(G.parameters(), 'results/G_para/')
# save paras of trained D
save_parameters(D.parameters(), 'results/D_para/')

# 7. visualize loss and samples generated by G

# load loss
def load_loss(path):
    tmp = pd.read_csv(path)
    iter_list = tmp.loc[:, tmp.columns == 'iteration'].to_numpy().tolist()
    G_loss = tmp.loc[:, tmp.columns == 'G_loss'].to_numpy().flatten().tolist()
    D_real_loss = tmp.loc[:, tmp.columns == 'D_real_loss'].to_numpy().flatten().tolist()
    D_fake_loss = tmp.loc[:, tmp.columns == 'D_fake_loss'].to_numpy().flatten().tolist()
    
    return iter_list, G_loss, D_real_loss, D_fake_loss

iter_list, G_loss_list, D_real_loss_list, D_fake_loss_list = load_loss('results/loss.csv')

# plot D and G loss
fig = plt.figure()
plt.plot(iter_list[::5], G_loss_list[::5], color = 'r', label='G_loss')
plt.plot(iter_list[::5], D_real_loss_list[::5], color = 'b', label='D_real_loss')
plt.plot(iter_list[::5], D_fake_loss_list[::5], color = 'g', label='D_fake_loss')
plt.xlabel('number of iteration')
plt.legend()
#plt.show()
fig.savefig('results/plot/loss_plot.png')

